{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_azIU3OGmsk",
    "outputId": "70bec597-dfe6-4671-b944-71cf3517440c"
   },
   "outputs": [],
   "source": [
    "# !pip install snowflake-connector-python neo4j spacy textblob gensim transformers umap-learn scipy networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xb77l3vWGkVD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (calinski_harabasz_score, davies_bouldin_score,\n",
    "                             silhouette_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Km5z9LaZUAOk"
   },
   "outputs": [],
   "source": [
    "class AdvancedMultiDimensionalClustering:\n",
    "    def __init__(self, snowflake_creds, neo4j_creds):\n",
    "        \"\"\"Initialize the clustering system\"\"\"\n",
    "        self.snowflake_conn = snowflake.connector.connect(**snowflake_creds)\n",
    "        self.neo4j_driver = GraphDatabase.driver(\n",
    "            neo4j_creds['uri'],\n",
    "            auth=(neo4j_creds['user'], neo4j_creds['password'])\n",
    "        )\n",
    "        self.vector_dim = 4096\n",
    "        self.initialize_configuration()\n",
    "\n",
    "    def initialize_configuration(self):\n",
    "        \"\"\"Initialize clustering configuration\"\"\"\n",
    "        self.clustering_config = {\n",
    "            'kmeans': {\n",
    "                'n_clusters_range': range(5, 51, 5),\n",
    "                'random_state': 42\n",
    "            },\n",
    "            'hierarchical': {\n",
    "                'n_clusters_range': range(5, 51, 5),\n",
    "                'linkage': 'ward'\n",
    "            },\n",
    "            'dbscan': {\n",
    "                'eps_range': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "                'min_samples_range': [5, 10, 15, 20]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def fetch_data(self):\n",
    "        \"\"\"Fetch data from Snowflake, including ATTRACTION_NAME\"\"\"\n",
    "        query = \"\"\"\n",
    "        WITH ReviewData AS (\n",
    "            SELECT\n",
    "                r.REVIEW_ID,\n",
    "                r.ATTRACTION_ID,\n",
    "                r.REVIEW_VECTOR,\n",
    "                r.RATING,\n",
    "                r.CITY,\n",
    "                r.STATE,\n",
    "                r.COUNTRY,\n",
    "                a.ATTRACTION_NAME,\n",
    "                a.PRIMARY_CATEGORY,\n",
    "                a.CATEGORY_ARRAY,\n",
    "                a.POPULARITY_TIER,\n",
    "                a.RATING_TIER,\n",
    "                a.WEIGHTED_RATING,\n",
    "                CASE\n",
    "                    WHEN r.RATING >= 4 THEN 'POSITIVE'\n",
    "                    WHEN r.RATING <= 2 THEN 'NEGATIVE'\n",
    "                    ELSE 'NEUTRAL'\n",
    "                END as SENTIMENT\n",
    "            FROM TRAVEL_GENIE.TRANSFORMED_DATA_TRANSFORMED.ATTRACTION_REVIEWS_VECTORIZED r\n",
    "            JOIN TRAVEL_GENIE.TRANSFORMED_DATA_TRANSFORMED.ATTRACTIONS a\n",
    "            ON r.ATTRACTION_ID = a.ATTRACTION_ID\n",
    "            WHERE r.REVIEW_VECTOR IS NOT NULL\n",
    "        )\n",
    "        SELECT * FROM ReviewData\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\"Fetching data from Snowflake...\")\n",
    "        cursor = self.snowflake_conn.cursor()\n",
    "        cursor.execute(query)\n",
    "\n",
    "        columns = [desc[0].lower() for desc in cursor.description]\n",
    "        data = cursor.fetchall()\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "        logger.info(f\"Fetched {len(df)} rows of data\")\n",
    "        return df\n",
    "\n",
    "    def process_vectors(self, data):\n",
    "        \"\"\"Process and validate vectors\"\"\"\n",
    "        logger.info(\"Processing review vectors...\")\n",
    "        vectors = []\n",
    "        valid_indices = []\n",
    "        errors = []\n",
    "\n",
    "        for idx, vec in enumerate(tqdm(data['review_vector'])):\n",
    "            try:\n",
    "                if isinstance(vec, str):\n",
    "                    cleaned_vec = vec.strip().replace('\\n', '').replace(' ', '')\n",
    "                    vector = np.array(json.loads(cleaned_vec))\n",
    "\n",
    "                    if len(vector) == self.vector_dim:\n",
    "                        vectors.append(vector)\n",
    "                        valid_indices.append(idx)\n",
    "                    else:\n",
    "                        errors.append(f\"Wrong dimension at index {idx}: {len(vector)}\")\n",
    "                else:\n",
    "                    errors.append(f\"Invalid type at index {idx}: {type(vec)}\")\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Error at index {idx}: {str(e)}\")\n",
    "\n",
    "        logger.info(f\"Processed {len(vectors)} valid vectors out of {len(data)}\")\n",
    "        if errors:\n",
    "            logger.warning(f\"Encountered {len(errors)} errors. First few: {errors[:5]}\")\n",
    "\n",
    "        return np.array(vectors), valid_indices\n",
    "\n",
    "    def create_feature_matrices(self, data, valid_indices):\n",
    "        \"\"\"Create comprehensive feature matrices\"\"\"\n",
    "        logger.info(\"Creating feature matrices...\")\n",
    "        features = {}\n",
    "        valid_data = data.iloc[valid_indices].copy()\n",
    "\n",
    "        # Basic features\n",
    "        features['category'] = pd.get_dummies(valid_data['primary_category']).values\n",
    "        features['location'] = pd.get_dummies(\n",
    "            valid_data[['city', 'state', 'country']].fillna('UNKNOWN')\n",
    "        ).values\n",
    "\n",
    "        # Advanced rating features\n",
    "        rating_features = []\n",
    "        rating_features.append(valid_data['rating'].fillna(0))\n",
    "        rating_features.append(valid_data['weighted_rating'].fillna(0))\n",
    "        rating_features.append(pd.get_dummies(valid_data['rating_tier']).values.T)\n",
    "        rating_features.append(pd.get_dummies(valid_data['popularity_tier']).values.T)\n",
    "        features['rating'] = np.vstack(rating_features).T\n",
    "\n",
    "        # Sentiment features\n",
    "        features['sentiment'] = pd.get_dummies(valid_data['sentiment']).values\n",
    "\n",
    "        # Location popularity features\n",
    "        location_stats = valid_data.groupby(['city', 'state']).agg({\n",
    "            'rating': ['mean', 'count']\n",
    "        }).reset_index()\n",
    "        location_stats.columns = ['city', 'state', 'loc_avg_rating', 'loc_review_count']\n",
    "        valid_data = valid_data.merge(location_stats, on=['city', 'state'], how='left')\n",
    "\n",
    "        features['location_stats'] = np.vstack([\n",
    "            valid_data['loc_avg_rating'].fillna(0),\n",
    "            np.log1p(valid_data['loc_review_count'].fillna(0))\n",
    "        ]).T\n",
    "\n",
    "        return features\n",
    "\n",
    "    def reduce_dimensions(self, vectors):\n",
    "        \"\"\"Enhanced dimension reduction\"\"\"\n",
    "        logger.info(\"Reducing dimensions...\")\n",
    "\n",
    "        # Initial scaling\n",
    "        scaled_vectors = StandardScaler().fit_transform(vectors)\n",
    "\n",
    "        # UMAP reduction\n",
    "        reducer = umap.UMAP(\n",
    "            n_components=100,\n",
    "            n_neighbors=30,\n",
    "            min_dist=0.1,\n",
    "            metric='cosine',\n",
    "            random_state=42\n",
    "        )\n",
    "        reduced = reducer.fit_transform(scaled_vectors)\n",
    "\n",
    "        logger.info(f\"Reduced dimensions from {vectors.shape[1]} to {reduced.shape[1]}\")\n",
    "        return reduced\n",
    "\n",
    "    def combine_features(self, reduced_vectors, feature_matrices):\n",
    "        \"\"\"Combine all features with weights\"\"\"\n",
    "        combined = np.hstack([\n",
    "            reduced_vectors * 0.5,  # Base vectors\n",
    "            StandardScaler().fit_transform(feature_matrices['category']) * 0.2,  # Categories\n",
    "            StandardScaler().fit_transform(feature_matrices['rating']) * 0.15,  # Ratings\n",
    "            StandardScaler().fit_transform(feature_matrices['location']) * 0.1,  # Location\n",
    "            StandardScaler().fit_transform(feature_matrices['sentiment']) * 0.05  # Sentiment\n",
    "        ])\n",
    "        return combined\n",
    "\n",
    "    def perform_clustering(self, vectors):\n",
    "        \"\"\"Enhanced multi-method clustering with debugging\"\"\"\n",
    "        logger.info(f\"Starting clustering with vector shape: {vectors.shape}\")\n",
    "        results = {}\n",
    "        scores = {}\n",
    "\n",
    "        # Adjust clustering parameters based on data size\n",
    "        n_samples = vectors.shape[0]\n",
    "        logger.info(f\"Number of samples: {n_samples}\")\n",
    "\n",
    "        # Dynamically set cluster ranges\n",
    "        min_clusters = max(2, int(n_samples/100))  # At least 2 clusters\n",
    "        max_clusters = min(int(n_samples/10), 50)  # No more than 50 or n_samples/10\n",
    "        step_size = max(1, int((max_clusters - min_clusters)/5))\n",
    "\n",
    "        self.clustering_config = {\n",
    "            'kmeans': {\n",
    "                'n_clusters_range': range(min_clusters, max_clusters + 1, step_size),\n",
    "                'random_state': 42\n",
    "            },\n",
    "            'hierarchical': {\n",
    "                'n_clusters_range': range(min_clusters, max_clusters + 1, step_size),\n",
    "                'linkage': 'ward'\n",
    "            }\n",
    "        }\n",
    "\n",
    "        logger.info(f\"Cluster range: {min_clusters} to {max_clusters}, step size: {step_size}\")\n",
    "\n",
    "        # K-means clustering\n",
    "        try:\n",
    "            logger.info(\"Attempting K-means clustering...\")\n",
    "            best_kmeans = None\n",
    "            best_kmeans_metrics = {}\n",
    "            best_kmeans_score = float('-inf')\n",
    "\n",
    "            for n_clusters in self.clustering_config['kmeans']['n_clusters_range']:\n",
    "                logger.info(f\"Trying K-means with {n_clusters} clusters\")\n",
    "\n",
    "                kmeans = KMeans(\n",
    "                    n_clusters=n_clusters,\n",
    "                    random_state=42,\n",
    "                    n_init=10,\n",
    "                    max_iter=300\n",
    "                )\n",
    "\n",
    "                labels = kmeans.fit_predict(vectors)\n",
    "\n",
    "                # Ensure we have at least 2 clusters\n",
    "                unique_labels = np.unique(labels)\n",
    "                if len(unique_labels) < 2:\n",
    "                    logger.warning(f\"K-means produced only {len(unique_labels)} clusters\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Compute multiple metrics\n",
    "                    silhouette = silhouette_score(vectors, labels)\n",
    "                    calinski_harabasz = calinski_harabasz_score(vectors, labels)\n",
    "                    davies_bouldin = davies_bouldin_score(vectors, labels)\n",
    "                    logger.info(f\"K-means with {n_clusters} clusters: silhouette = {silhouette}, calinski_harabasz = {calinski_harabasz}, davies_bouldin = {davies_bouldin}\")\n",
    "\n",
    "                    # Use silhouette score as the main criterion for best clustering\n",
    "                    if silhouette > best_kmeans_score:\n",
    "                        best_kmeans = labels\n",
    "                        best_kmeans_score = silhouette\n",
    "                        best_kmeans_metrics = {\n",
    "                            'silhouette_score': silhouette,\n",
    "                            'calinski_harabasz_score': calinski_harabasz,\n",
    "                            'davies_bouldin_score': davies_bouldin\n",
    "                        }\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error calculating metrics for K-means: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if best_kmeans is not None:\n",
    "                results['kmeans'] = best_kmeans\n",
    "                scores['kmeans'] = best_kmeans_metrics\n",
    "                logger.info(f\"K-means clustering successful with silhouette score: {best_kmeans_score}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in K-means clustering: {str(e)}\")\n",
    "\n",
    "        # Hierarchical clustering\n",
    "        try:\n",
    "            logger.info(\"Attempting hierarchical clustering...\")\n",
    "            best_hierarchical = None\n",
    "            best_hierarchical_metrics = {}\n",
    "            best_hierarchical_score = float('-inf')\n",
    "\n",
    "            for n_clusters in self.clustering_config['hierarchical']['n_clusters_range']:\n",
    "                logger.info(f\"Trying hierarchical with {n_clusters} clusters\")\n",
    "\n",
    "                hierarchical = AgglomerativeClustering(\n",
    "                    n_clusters=n_clusters,\n",
    "                    linkage='ward'\n",
    "                )\n",
    "\n",
    "                labels = hierarchical.fit_predict(vectors)\n",
    "\n",
    "                # Ensure we have at least 2 clusters\n",
    "                unique_labels = np.unique(labels)\n",
    "                if len(unique_labels) < 2:\n",
    "                    logger.warning(f\"Hierarchical produced only {len(unique_labels)} clusters\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Compute multiple metrics\n",
    "                    silhouette = silhouette_score(vectors, labels)\n",
    "                    calinski_harabasz = calinski_harabasz_score(vectors, labels)\n",
    "                    davies_bouldin = davies_bouldin_score(vectors, labels)\n",
    "                    logger.info(f\"Hierarchical with {n_clusters} clusters: silhouette = {silhouette}, calinski_harabasz = {calinski_harabasz}, davies_bouldin = {davies_bouldin}\")\n",
    "\n",
    "                    # Use silhouette score as the main criterion for best clustering\n",
    "                    if silhouette > best_hierarchical_score:\n",
    "                        best_hierarchical = labels\n",
    "                        best_hierarchical_score = silhouette\n",
    "                        best_hierarchical_metrics = {\n",
    "                            'silhouette_score': silhouette,\n",
    "                            'calinski_harabasz_score': calinski_harabasz,\n",
    "                            'davies_bouldin_score': davies_bouldin\n",
    "                        }\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error calculating metrics for hierarchical: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if best_hierarchical is not None:\n",
    "                results['hierarchical'] = best_hierarchical\n",
    "                scores['hierarchical'] = best_hierarchical_metrics\n",
    "                logger.info(f\"Hierarchical clustering successful with silhouette score: {best_hierarchical_score}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in hierarchical clustering: {str(e)}\")\n",
    "\n",
    "        # If both methods fail, try DBSCAN as fallback\n",
    "        if not results:\n",
    "            try:\n",
    "                logger.info(\"Attempting DBSCAN as fallback...\")\n",
    "                dbscan = DBSCAN(\n",
    "                    eps=0.5,\n",
    "                    min_samples=5,\n",
    "                    metric='euclidean'\n",
    "                )\n",
    "                labels = dbscan.fit_predict(vectors)\n",
    "\n",
    "                # Check if DBSCAN produced valid clusters\n",
    "                unique_labels = np.unique(labels[labels >= 0])\n",
    "                if len(unique_labels) >= 2:\n",
    "                    results['dbscan'] = labels\n",
    "                    try:\n",
    "                        # Compute multiple metrics\n",
    "                        silhouette = silhouette_score(vectors, labels, metric='euclidean')\n",
    "                        calinski_harabasz = calinski_harabasz_score(vectors, labels)\n",
    "                        davies_bouldin = davies_bouldin_score(vectors, labels)\n",
    "                        scores['dbscan'] = {\n",
    "                            'silhouette_score': silhouette,\n",
    "                            'calinski_harabasz_score': calinski_harabasz,\n",
    "                            'davies_bouldin_score': davies_bouldin\n",
    "                        }\n",
    "                        logger.info(\"DBSCAN clustering successful\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error calculating metrics for DBSCAN: {str(e)}\")\n",
    "                        scores['dbscan'] = {\n",
    "                            'silhouette_score': None,\n",
    "                            'calinski_harabasz_score': None,\n",
    "                            'davies_bouldin_score': None\n",
    "                        }\n",
    "                else:\n",
    "                    logger.warning(\"DBSCAN failed to produce valid clusters\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in DBSCAN clustering: {str(e)}\")\n",
    "\n",
    "        # Final check\n",
    "        if not results:\n",
    "            # If all methods fail, create simple binary clustering using median split\n",
    "            logger.warning(\"All clustering methods failed, using median split...\")\n",
    "            try:\n",
    "                # Use first principal component for splitting\n",
    "                pca = PCA(n_components=1)\n",
    "                projected = pca.fit_transform(vectors)\n",
    "                median = np.median(projected)\n",
    "                labels = (projected > median).astype(int).flatten()\n",
    "\n",
    "                results['binary'] = labels\n",
    "                scores['binary'] = {\n",
    "                    'silhouette_score': None,\n",
    "                    'calinski_harabasz_score': None,\n",
    "                    'davies_bouldin_score': None\n",
    "                }  # No real score for this method\n",
    "                logger.info(\"Created binary split as last resort\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error creating binary split: {str(e)}\")\n",
    "                raise ValueError(\"No clustering method succeeded\")\n",
    "\n",
    "        logger.info(f\"Clustering completed with methods: {list(results.keys())}\")\n",
    "        return results, scores\n",
    "\n",
    "    def analyze_clusters(self, clustering_results, data, valid_indices):\n",
    "        \"\"\"Enhanced cluster analysis with comprehensive metrics\"\"\"\n",
    "        logger.info(\"Analyzing clusters...\")\n",
    "        analysis = {}\n",
    "\n",
    "        # Get valid data\n",
    "        valid_data = data.iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "        for method, labels in clustering_results.items():\n",
    "            try:\n",
    "                logger.info(f\"Analyzing {method} clusters...\")\n",
    "                clusters_analysis = {}\n",
    "\n",
    "                # Get unique valid labels (excluding noise points marked as -1)\n",
    "                valid_labels = set(labels[labels >= 0])\n",
    "                logger.info(f\"Found {len(valid_labels)} valid clusters for {method}\")\n",
    "\n",
    "                for label in valid_labels:\n",
    "                    # Get data for this cluster\n",
    "                    mask = labels == label\n",
    "                    cluster_data = valid_data[mask]\n",
    "\n",
    "                    if len(cluster_data) == 0:\n",
    "                        logger.warning(f\"Empty cluster found for {method}, label {label}\")\n",
    "                        continue\n",
    "\n",
    "                    # Initialize cluster info dictionary\n",
    "                    cluster_info = {}\n",
    "\n",
    "                    # Basic cluster statistics\n",
    "                    cluster_info['size'] = int(len(cluster_data))\n",
    "\n",
    "                    # Rating analysis\n",
    "                    cluster_info['rating_analysis'] = {\n",
    "                        'mean': float(cluster_data['rating'].mean()),\n",
    "                        'median': float(cluster_data['rating'].median()),\n",
    "                        'std': float(cluster_data['rating'].std()),\n",
    "                        'min': float(cluster_data['rating'].min()),\n",
    "                        'max': float(cluster_data['rating'].max()),\n",
    "                        'weighted_avg': float(cluster_data['weighted_rating'].mean())\n",
    "                    }\n",
    "\n",
    "                    # Category distribution\n",
    "                    primary_category_counts = cluster_data['primary_category'].value_counts()\n",
    "                    cluster_info['category_analysis'] = {\n",
    "                        'primary_categories': primary_category_counts.to_dict(),\n",
    "                        'category_entropy': float(entropy(primary_category_counts.values)),\n",
    "                        'dominant_category': primary_category_counts.idxmax()\n",
    "                    }\n",
    "\n",
    "                    # Sentiment distribution\n",
    "                    sentiment_counts = cluster_data['sentiment'].value_counts()\n",
    "                    cluster_info['sentiment_analysis'] = {\n",
    "                        'distribution': sentiment_counts.to_dict(),\n",
    "                        'sentiment_entropy': float(entropy(sentiment_counts.values)),\n",
    "                        'dominant_sentiment': sentiment_counts.idxmax()\n",
    "                    }\n",
    "\n",
    "                    # Location analysis\n",
    "                    cluster_info['location_analysis'] = {\n",
    "                        'cities': {\n",
    "                            'distribution': cluster_data['city'].value_counts().head(10).to_dict(),\n",
    "                            'unique_count': int(cluster_data['city'].nunique()),\n",
    "                            'top_city': cluster_data['city'].mode().iloc[0]\n",
    "                        },\n",
    "                        'states': {\n",
    "                            'distribution': cluster_data['state'].value_counts().head(10).to_dict(),\n",
    "                            'unique_count': int(cluster_data['state'].nunique()),\n",
    "                            'top_state': cluster_data['state'].mode().iloc[0]\n",
    "                        },\n",
    "                        'countries': {\n",
    "                            'distribution': cluster_data['country'].value_counts().head(5).to_dict(),\n",
    "                            'unique_count': int(cluster_data['country'].nunique()),\n",
    "                            'top_country': cluster_data['country'].mode().iloc[0]\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    # Popularity metrics\n",
    "                    popularity_tier_counts = cluster_data['popularity_tier'].value_counts()\n",
    "                    cluster_info['popularity_analysis'] = {\n",
    "                        'tier_distribution': popularity_tier_counts.to_dict(),\n",
    "                        'dominant_tier': popularity_tier_counts.idxmax(),\n",
    "                        'avg_weighted_rating': float(cluster_data['weighted_rating'].mean())\n",
    "                    }\n",
    "\n",
    "                    # Attraction analysis (Include ATTRACTION_NAME)\n",
    "                    attraction_counts = cluster_data['attraction_name'].value_counts()\n",
    "                    cluster_info['attraction_analysis'] = {\n",
    "                        'attractions': attraction_counts.to_dict(),\n",
    "                        'unique_count': int(cluster_data['attraction_name'].nunique()),\n",
    "                        'top_attractions': attraction_counts.head(10).to_dict()\n",
    "                    }\n",
    "\n",
    "                    # Cluster quality metrics\n",
    "                    cluster_info['quality_metrics'] = {\n",
    "                        'category_cohesion': float(1 - entropy(primary_category_counts.values / primary_category_counts.values.sum())),\n",
    "                        'sentiment_cohesion': float(1 - entropy(sentiment_counts.values / sentiment_counts.values.sum())),\n",
    "                        'rating_consistency': float(1 / (1 + cluster_data['rating'].std())),\n",
    "                        'location_diversity': float(entropy(cluster_data['city'].value_counts(normalize=True).values))\n",
    "                    }\n",
    "\n",
    "                    # Calculate overall cluster quality score\n",
    "                    quality_metrics = cluster_info['quality_metrics']\n",
    "                    cluster_info['overall_quality_score'] = float(np.mean([\n",
    "                        quality_metrics['category_cohesion'],\n",
    "                        quality_metrics['sentiment_cohesion'],\n",
    "                        quality_metrics['rating_consistency'],\n",
    "                        1 - (quality_metrics['location_diversity'] / np.log(max(2, cluster_data['city'].nunique())))\n",
    "                    ]))\n",
    "\n",
    "                    # Assign cluster info to clusters_analysis\n",
    "                    clusters_analysis[label] = cluster_info\n",
    "\n",
    "                if clusters_analysis:\n",
    "                    # Compute summary statistics\n",
    "                    sizes = [info['size'] for info in clusters_analysis.values()]\n",
    "                    quality_scores = [info['overall_quality_score'] for info in clusters_analysis.values()]\n",
    "                    analysis[method] = {\n",
    "                        'clusters': clusters_analysis,\n",
    "                        'summary': {\n",
    "                            'total_clusters': len(clusters_analysis),\n",
    "                            'total_points': sum(sizes),\n",
    "                            'avg_cluster_size': float(np.mean(sizes)),\n",
    "                            'std_cluster_size': float(np.std(sizes)),\n",
    "                            'min_cluster_size': min(sizes),\n",
    "                            'max_cluster_size': max(sizes),\n",
    "                            'avg_quality_score': float(np.mean(quality_scores))\n",
    "                        }\n",
    "                    }\n",
    "                    logger.info(f\"Completed analysis for {method} clustering\")\n",
    "                else:\n",
    "                    logger.warning(f\"No valid clusters found for {method}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error analyzing {method} clusters: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if not analysis:\n",
    "            raise ValueError(\"No clusters could be analyzed\")\n",
    "\n",
    "        logger.info(\"Cluster analysis completed successfully\")\n",
    "        return analysis\n",
    "\n",
    "    def calculate_cluster_similarity(self, cluster1_info, cluster2_info):\n",
    "        \"\"\"Calculate similarity between two clusters using cosine similarity\"\"\"\n",
    "        # Extract centroid vectors\n",
    "        centroid1 = cluster1_info['centroid_vector']\n",
    "        centroid2 = cluster2_info['centroid_vector']\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        dot_product = np.dot(centroid1, centroid2)\n",
    "        norm1 = np.linalg.norm(centroid1)\n",
    "        norm2 = np.linalg.norm(centroid2)\n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "        similarity = dot_product / (norm1 * norm2)\n",
    "        return similarity\n",
    "\n",
    "    def update_cluster_centroids(self, clustering_results, combined_features):\n",
    "        \"\"\"Compute and store centroid vectors for each cluster\"\"\"\n",
    "        logger.info(\"Computing cluster centroids...\")\n",
    "        centroids = {}\n",
    "        for method, labels in clustering_results.items():\n",
    "            centroids[method] = {}\n",
    "            unique_labels = set(labels[labels >= 0])\n",
    "            for label in unique_labels:\n",
    "                mask = labels == label\n",
    "                cluster_vectors = combined_features[mask]\n",
    "                centroid = np.mean(cluster_vectors, axis=0)\n",
    "                centroids[method][label] = centroid\n",
    "        return centroids\n",
    "\n",
    "    def clean_neo4j(self):\n",
    "        \"\"\"Clean existing cluster data from Neo4j\"\"\"\n",
    "        logger.info(\"Cleaning existing cluster data...\")\n",
    "        with self.neo4j_driver.session() as session:\n",
    "            session.run(\"MATCH ()-[r:IN_CLUSTER]->() DELETE r\")\n",
    "            session.run(\"MATCH ()-[r:RELATED_TO]->() DELETE r\")\n",
    "            session.run(\"MATCH (c:Cluster) DELETE c\")\n",
    "            session.run(\"MATCH (a:Attraction) DELETE a\")\n",
    "            logger.info(\"Existing cluster data cleaned\")\n",
    "\n",
    "    def update_neo4j(self, clustering_results, analysis, data, valid_indices):\n",
    "        \"\"\"Update Neo4j with clustering results, including attractions\"\"\"\n",
    "        import json  # Ensure this is imported\n",
    "        logger.info(\"Updating Neo4j with results...\")\n",
    "        self.clean_neo4j()\n",
    "\n",
    "        valid_data = data.iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "        def convert_numpy_types(obj):\n",
    "            if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_numpy_types(v) for v in obj]\n",
    "            else:\n",
    "                return obj\n",
    "\n",
    "        with self.neo4j_driver.session() as session:\n",
    "            # Create cluster nodes\n",
    "            for method, labels in clustering_results.items():\n",
    "                for label in set(labels[labels >= 0]):\n",
    "                    cluster_info = analysis[method]['clusters'][label]\n",
    "\n",
    "                    session.run(\"\"\"\n",
    "                    CREATE (c:Cluster {\n",
    "                        cluster_id: $cluster_id,\n",
    "                        method: $method,\n",
    "                        size: $size,\n",
    "                        avg_rating: $avg_rating,\n",
    "                        categories: $categories,\n",
    "                        sentiments: $sentiment_dist,\n",
    "                        locations: $locations,\n",
    "                        metrics: $metrics,\n",
    "                        popularity: $popularity,\n",
    "                        attractions: $attractions,\n",
    "                        created_at: datetime()\n",
    "                    })\n",
    "                    \"\"\",\n",
    "                    cluster_id=f\"{method}_{label}\",\n",
    "                    method=method,\n",
    "                    size=int(cluster_info['size']),\n",
    "                    avg_rating=float(cluster_info['rating_analysis']['mean']),\n",
    "                    categories=json.dumps(convert_numpy_types(cluster_info['category_analysis']['primary_categories'])),\n",
    "                    sentiment_dist=json.dumps(convert_numpy_types(cluster_info['sentiment_analysis']['distribution'])),\n",
    "                    locations=json.dumps(convert_numpy_types(cluster_info['location_analysis'])),\n",
    "                    metrics=json.dumps(convert_numpy_types(cluster_info['quality_metrics'])),\n",
    "                    popularity=json.dumps(convert_numpy_types(cluster_info['popularity_analysis'])),\n",
    "                    attractions=json.dumps(convert_numpy_types(cluster_info['attraction_analysis']['top_attractions'])),\n",
    "                    )\n",
    "\n",
    "            # Create attraction nodes and relationships\n",
    "            logger.info(\"Creating attraction nodes and relationships...\")\n",
    "            for idx, row in valid_data.iterrows():\n",
    "                attraction_id = row['attraction_id']\n",
    "                attraction_name = row['attraction_name']\n",
    "                cluster_labels = {}\n",
    "                for method, labels in clustering_results.items():\n",
    "                    label = labels[idx]\n",
    "                    if label >= 0:\n",
    "                        cluster_id = f\"{method}_{label}\"\n",
    "                        cluster_labels[method] = cluster_id\n",
    "\n",
    "                        # Create attraction node if not exists\n",
    "                        session.run(\"\"\"\n",
    "                        MERGE (a:Attraction {attraction_id: $attraction_id})\n",
    "                        SET a.name = $attraction_name\n",
    "                        \"\"\",\n",
    "                        attraction_id=attraction_id,\n",
    "                        attraction_name=attraction_name\n",
    "                        )\n",
    "\n",
    "                        # Create relationship to cluster\n",
    "                        session.run(\"\"\"\n",
    "                        MATCH (a:Attraction {attraction_id: $attraction_id})\n",
    "                        MATCH (c:Cluster {cluster_id: $cluster_id})\n",
    "                        MERGE (a)-[:IN_CLUSTER {\n",
    "                            method: $method,\n",
    "                            created_at: datetime()\n",
    "                        }]->(c)\n",
    "                        \"\"\",\n",
    "                        attraction_id=attraction_id,\n",
    "                        cluster_id=cluster_id,\n",
    "                        method=method\n",
    "                        )\n",
    "\n",
    "    def create_cluster_relationships(self, session, clustering_results, analysis):\n",
    "        \"\"\"Create relationships between clusters using cosine similarity\"\"\"\n",
    "        logger.info(\"Creating cluster relationships...\")\n",
    "\n",
    "        for method, labels in clustering_results.items():\n",
    "            unique_labels = set(labels[labels >= 0])\n",
    "\n",
    "            clusters_info = analysis[method]['clusters']\n",
    "            for label1 in unique_labels:\n",
    "                cluster1_info = clusters_info[label1]\n",
    "\n",
    "                for label2 in unique_labels:\n",
    "                    if label1 < label2:  # Only process each pair once\n",
    "                        cluster2_info = clusters_info[label2]\n",
    "\n",
    "                        similarity = self.calculate_cluster_similarity(\n",
    "                            cluster1_info,\n",
    "                            cluster2_info\n",
    "                        )\n",
    "\n",
    "                        if similarity > 0.3:\n",
    "                            session.run(\"\"\"\n",
    "                            MATCH (c1:Cluster {cluster_id: $id1})\n",
    "                            MATCH (c2:Cluster {cluster_id: $id2})\n",
    "                            CREATE (c1)-[:RELATED_TO {\n",
    "                                similarity: $similarity,\n",
    "                                method: $method,\n",
    "                                created_at: datetime()\n",
    "                            }]->(c2)\n",
    "                            \"\"\",\n",
    "                            id1=f\"{method}_{label1}\",\n",
    "                            id2=f\"{method}_{label2}\",\n",
    "                            similarity=float(similarity),\n",
    "                            method=method\n",
    "                            )\n",
    "\n",
    "    def process_and_cluster(self):\n",
    "        \"\"\"Main processing method that calls everything in order\"\"\"\n",
    "        try:\n",
    "            # Fetch and process data\n",
    "            data = self.fetch_data()\n",
    "            vectors, valid_indices = self.process_vectors(data)\n",
    "\n",
    "            # Feature engineering\n",
    "            features = self.create_feature_matrices(data, valid_indices)\n",
    "            reduced_vectors = self.reduce_dimensions(vectors)\n",
    "            combined_features = self.combine_features(reduced_vectors, features)\n",
    "\n",
    "            # Clustering and analysis\n",
    "            clustering_results, scores = self.perform_clustering(combined_features)\n",
    "\n",
    "            # Update cluster centroids\n",
    "            centroids = self.update_cluster_centroids(clustering_results, combined_features)\n",
    "\n",
    "            # Incorporate centroids into analysis\n",
    "            analysis = self.analyze_clusters(clustering_results, data, valid_indices)\n",
    "            for method in analysis.keys():\n",
    "                for label in analysis[method]['clusters'].keys():\n",
    "                    analysis[method]['clusters'][label]['centroid_vector'] = centroids[method][label]\n",
    "\n",
    "            # Neo4j updates\n",
    "            self.update_neo4j(clustering_results, analysis, data, valid_indices)\n",
    "\n",
    "            # Create relationships in Neo4j\n",
    "            with self.neo4j_driver.session() as session:\n",
    "                self.create_cluster_relationships(session, clustering_results, analysis)\n",
    "\n",
    "            return {\n",
    "                'clustering_results': clustering_results,\n",
    "                'scores': scores,\n",
    "                'analysis': analysis,\n",
    "                'stats': {\n",
    "                    'total_vectors': len(vectors),\n",
    "                    'valid_vectors': len(valid_indices),\n",
    "                    'dimensions': {\n",
    "                        'original': self.vector_dim,\n",
    "                        'reduced': reduced_vectors.shape[1],\n",
    "                        'final': combined_features.shape[1]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during clustering: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.snowflake_conn.close()\n",
    "            self.neo4j_driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daNj3fAOUALR",
    "outputId": "cfea7a31-e485-4bab-963a-1773351f0d20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.12.3, Python Version: 3.12.7, Platform: Windows-11-10.0.22631-SP0\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "INFO:__main__:Fetching data from Snowflake...\n",
      "INFO:snowflake.connector.cursor:Number of results in first chunk: 7\n",
      "INFO:__main__:Fetched 5635 rows of data\n",
      "INFO:__main__:Processing review vectors...\n",
      "100%|██████████| 5635/5635 [00:08<00:00, 694.60it/s]\n",
      "INFO:__main__:Processed 5635 valid vectors out of 5635\n",
      "INFO:__main__:Creating feature matrices...\n",
      "INFO:__main__:Reducing dimensions...\n",
      "c:\\Users\\darsh\\Desktop\\DAMG7374\\.venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "INFO:__main__:Reduced dimensions from 4096 to 100\n",
      "INFO:__main__:Starting clustering with vector shape: (5635, 142)\n",
      "INFO:__main__:Number of samples: 5635\n",
      "INFO:__main__:Cluster range: 56 to 50, step size: 1\n",
      "INFO:__main__:Attempting K-means clustering...\n",
      "INFO:__main__:Attempting hierarchical clustering...\n",
      "INFO:__main__:Attempting DBSCAN as fallback...\n",
      "INFO:__main__:DBSCAN clustering successful\n",
      "INFO:__main__:Clustering completed with methods: ['dbscan']\n",
      "INFO:__main__:Computing cluster centroids...\n",
      "INFO:__main__:Analyzing clusters...\n",
      "INFO:__main__:Analyzing dbscan clusters...\n",
      "INFO:__main__:Found 118 valid clusters for dbscan\n",
      "INFO:__main__:Completed analysis for dbscan clustering\n",
      "INFO:__main__:Cluster analysis completed successfully\n",
      "INFO:__main__:Updating Neo4j with results...\n",
      "INFO:__main__:Cleaning existing cluster data...\n",
      "INFO:__main__:Existing cluster data cleaned\n",
      "INFO:__main__:Creating attraction nodes and relationships...\n",
      "INFO:__main__:Creating cluster relationships...\n",
      "INFO:snowflake.connector.connection:closed\n",
      "INFO:snowflake.connector.connection:No async queries seem to be running, deleting session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Clustering Results Summary ===\n",
      "Total vectors processed: 5635\n",
      "Valid vectors used: 5635\n",
      "\n",
      "DBSCAN Clustering Scores:\n",
      "silhouette_score: 0.3031\n",
      "calinski_harabasz_score: 214.2646\n",
      "davies_bouldin_score: 1.2695\n",
      "\n",
      "Cluster Sizes:\n",
      "\n",
      "DBSCAN:\n",
      "Number of clusters: 118\n",
      "Average cluster size: 44.12\n"
     ]
    }
   ],
   "source": [
    "# Set up credentials\n",
    "snowflake_creds = {\n",
    "      'user': os.getenv('SNOWFLAKE_USER'),\n",
    "      'password': os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "      'account': os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "      'warehouse': os.getenv('SNOWFLAKE_WAREHOUSE'),\n",
    "      'database': os.getenv('SNOWFLAKE_DATABASE'),\n",
    "      'schema': os.getenv('SNOWFLAKE_SCHEMA')\n",
    "}\n",
    "\n",
    "neo4j_creds = {\n",
    "     'uri': os.getenv('NEO4J_URI'),\n",
    "     'user': os.getenv('NEO4J_USERNAME'),\n",
    "     'password': os.getenv('NEO4J_PASSWORD')\n",
    "}\n",
    "\n",
    "# Initialize and run clustering\n",
    "clustering = AdvancedMultiDimensionalClustering(snowflake_creds, neo4j_creds)\n",
    "results = clustering.process_and_cluster()\n",
    "\n",
    "# Print results\n",
    "print(\"=== Clustering Results Summary ===\")\n",
    "print(f\"Total vectors processed: {results['stats']['total_vectors']}\")\n",
    "print(f\"Valid vectors used: {results['stats']['valid_vectors']}\")\n",
    "\n",
    "# Clustering Scores\n",
    "for method, metrics in results['scores'].items():\n",
    "    print(f\"\\n{method.upper()} Clustering Scores:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        if metric_value is not None:\n",
    "            print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric_name}: Not computed\")\n",
    "\n",
    "# Cluster Sizes\n",
    "print(\"\\nCluster Sizes:\")\n",
    "for method, method_info in results['analysis'].items():\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    clusters = method_info['clusters']\n",
    "    sizes = [info['size'] for info in clusters.values()]\n",
    "    print(f\"Number of clusters: {len(clusters)}\")\n",
    "    print(f\"Average cluster size: {sum(sizes)/len(sizes):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxNez3ecUAIc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyJeoTcPUAFr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTk_8HGrUACR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
